{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2df8d0-ce9c-40e4-a1e8-0a5089eb951d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch_snippets in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (0.560)\n",
      "Requirement already satisfied: plum-dispatch in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (2.6.0)\n",
      "Requirement already satisfied: fastcore in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (1.8.15)\n",
      "Requirement already satisfied: matplotlib in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (3.10.6)\n",
      "Requirement already satisfied: Pillow in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (11.0.0)\n",
      "Requirement already satisfied: dill in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (0.4.0)\n",
      "Requirement already satisfied: loguru in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (0.7.3)\n",
      "Requirement already satisfied: numpy in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (2.1.2)\n",
      "Requirement already satisfied: pandas in /opt/tljh/user/lib/python3.12/site-packages (from torch_snippets) (2.2.3)\n",
      "Requirement already satisfied: tqdm in /opt/tljh/user/lib/python3.12/site-packages (from torch_snippets) (4.66.5)\n",
      "Requirement already satisfied: rich in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (14.2.0)\n",
      "Requirement already satisfied: PyYAML in /opt/tljh/user/lib/python3.12/site-packages (from torch_snippets) (6.0.2)\n",
      "Requirement already satisfied: catalogue in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (2.0.10)\n",
      "Requirement already satisfied: confection in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (0.1.5)\n",
      "Requirement already satisfied: pydantic in /opt/tljh/user/lib/python3.12/site-packages (from torch_snippets) (2.10.4)\n",
      "Requirement already satisfied: typing in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (3.7.4.3)\n",
      "Requirement already satisfied: srsly in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (2.5.1)\n",
      "Requirement already satisfied: typing_extensions in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (4.12.2)\n",
      "Requirement already satisfied: wasabi in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (1.1.3)\n",
      "Requirement already satisfied: jsonlines in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (4.0.0)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (0.4.0)\n",
      "Requirement already satisfied: xmltodict in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (1.0.2)\n",
      "Requirement already satisfied: fuzzywuzzy in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (0.18.0)\n",
      "Requirement already satisfied: nltk in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (3.9.2)\n",
      "Requirement already satisfied: python-Levenshtein in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (0.27.3)\n",
      "Requirement already satisfied: pre-commit in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (4.3.0)\n",
      "Requirement already satisfied: icecream in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (2.1.8)\n",
      "Requirement already satisfied: mergedeep in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (1.3.4)\n",
      "Requirement already satisfied: deepdiff in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (8.6.1)\n",
      "Requirement already satisfied: typer in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (0.20.0)\n",
      "Requirement already satisfied: torch in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (0.23.0+cu128)\n",
      "Requirement already satisfied: lovely_tensors in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch_snippets) (0.1.19)\n",
      "Requirement already satisfied: six in /opt/tljh/user/lib/python3.12/site-packages (from imgaug>=0.4.0->torch_snippets) (1.17.0)\n",
      "Requirement already satisfied: scipy in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from imgaug>=0.4.0->torch_snippets) (1.16.2)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from imgaug>=0.4.0->torch_snippets) (0.25.2)\n",
      "Requirement already satisfied: opencv-python in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from imgaug>=0.4.0->torch_snippets) (4.12.0.88)\n",
      "Requirement already satisfied: imageio in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from imgaug>=0.4.0->torch_snippets) (2.37.2)\n",
      "Requirement already satisfied: Shapely in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from imgaug>=0.4.0->torch_snippets) (2.1.2)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (3.3)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (2025.10.16)\n",
      "Requirement already satisfied: packaging>=21 in /opt/tljh/user/lib/python3.12/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (0.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic->torch_snippets) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic->torch_snippets) (2.27.2)\n",
      "Requirement already satisfied: orderly-set<6,>=5.4.1 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from deepdiff->torch_snippets) (5.5.0)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /opt/tljh/user/lib/python3.12/site-packages (from icecream->torch_snippets) (0.4.6)\n",
      "Requirement already satisfied: pygments>=2.2.0 in /opt/tljh/user/lib/python3.12/site-packages (from icecream->torch_snippets) (2.18.0)\n",
      "Requirement already satisfied: executing>=2.1.0 in /opt/tljh/user/lib/python3.12/site-packages (from icecream->torch_snippets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /opt/tljh/user/lib/python3.12/site-packages (from icecream->torch_snippets) (3.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/tljh/user/lib/python3.12/site-packages (from jsonlines->torch_snippets) (24.3.0)\n",
      "Requirement already satisfied: lovely-numpy>=0.2.16 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from lovely_tensors->torch_snippets) (0.2.17)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from matplotlib->torch_snippets) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from matplotlib->torch_snippets) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from matplotlib->torch_snippets) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from matplotlib->torch_snippets) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from matplotlib->torch_snippets) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/tljh/user/lib/python3.12/site-packages (from matplotlib->torch_snippets) (2.9.0.post0)\n",
      "Requirement already satisfied: click in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from nltk->torch_snippets) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from nltk->torch_snippets) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from nltk->torch_snippets) (2025.11.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->torch_snippets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->torch_snippets) (2024.2)\n",
      "Requirement already satisfied: beartype>=0.16.2 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from plum-dispatch->torch_snippets) (0.22.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from rich->torch_snippets) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->torch_snippets) (0.1.2)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from pre-commit->torch_snippets) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from pre-commit->torch_snippets) (2.6.15)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from pre-commit->torch_snippets) (1.9.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from pre-commit->torch_snippets) (20.35.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (0.4.0)\n",
      "Requirement already satisfied: filelock<4,>=3.12.2 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (3.13.1)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/tljh/user/lib/python3.12/site-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (4.3.6)\n",
      "Requirement already satisfied: Levenshtein==0.27.3 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from python-Levenshtein->torch_snippets) (0.27.3)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from Levenshtein==0.27.3->python-Levenshtein->torch_snippets) (3.14.3)\n",
      "Requirement already satisfied: setuptools in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from torch->torch_snippets) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch->torch_snippets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from jinja2->torch->torch_snippets) (2.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages (from typer->torch_snippets) (1.5.4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "!pip install torch_snippets\n",
    "from torch_snippets import *\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models import vgg16_bn\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c17c668-92a6-4470-b04f-95c1c3280a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DOWNLOAD YOUR OWN DATASET AND TRY TO FIT\n",
    "\n",
    "# if not os.path.exists('dataset1'):\n",
    "#     !wget -q https://www.dropbox.com/s/0pigmmmynbf9xwq/dataset1.zip 'need to check the link'\n",
    "#     !unzip -q dataset1.zip\n",
    "#     !rm dataset1.zip\n",
    "#     !pip install -q pytorch_model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e27108-0625-4f4d-b6ee-edf295f532dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Params dictionary\n",
    "class Params(object):\n",
    "    def __init__(self, batch_size, test_batch_size, epochs, lr, seed, cuda, log_interval):\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.cuda = 'cuda' if cuda and torch.cuda.is_available() else 'cpu'\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "# Configure args\n",
    "args = Params(8, 2, 5, 1e-3, 1, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b457563-8ac7-4a00-8677-51c521e0492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "  return transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(\n",
    "                                 [0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225]\n",
    "                                 )\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ca1d00-ebdb-47a1-8e36-f16a674f379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SegmentationData(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.items = stems(f'dataset1/images_prepped_{split}')\n",
    "        self.split = split                                     # Store the split information\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve an image and its corresponding mask based on the index 'idx'\n",
    "        image = read(f'dataset1/images_prepped_{self.split}/{self.items[idx]}.png', 1)\n",
    "        image = cv.resize(image, (224,224))\n",
    "\n",
    "        mask = read(f'dataset1/annotations_prepped_{self.split}/{self.items[idx]}.png', 0)\n",
    "        mask = cv.resize(mask, (224,224))\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def choose(self): return self[randint(len(self))]  # Randomly select and return one image and mask pair from the dataset\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # Custom collate function to combine a batch of images and masks\n",
    "        # Unzip the batch into images and masks\n",
    "        ims, masks = list(zip(*batch))\n",
    "        \n",
    "        # Transform the images: Normalize and convert to tensor, then stack them into a single tensor\n",
    "\n",
    "        ims = torch.cat([get_transforms()(im.copy()/255.)[None] for im in ims]).float().to(args.cuda)\n",
    "\n",
    "        # Convert masks to tensors, stack them into a single tensor and cast to long type\n",
    "\n",
    "        ce_masks = torch.cat([torch.Tensor(mask[None]) for mask in masks]).long().to(args.cuda)\n",
    "\n",
    "        return ims, ce_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd5b27-acf7-4778-afec-3ed20c073bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    trn_ds = SegmentationData('train')\n",
    "    val_ds = SegmentationData('test')\n",
    "\n",
    "    trn_dl = DataLoader(trn_ds, batch_size=args.batch_size, shuffle=True, collate_fn=trn_ds.collate_fn)\n",
    "    val_dl = DataLoader(val_ds, batch_size=args.test_batch_size, shuffle=True, collate_fn=val_ds.collate_fn)\n",
    "\n",
    "    return trn_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63afa8-2756-4ffe-b755-ffde95b74d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl, val_dl = get_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d43cbb-432d-4ddc-b889-544fa78fb7bf",
   "metadata": {},
   "source": [
    "# U Net Architecture\n",
    "\n",
    "- U-Net is a convolutional neural network architecture primarily used for image segmentation tasks. It consists of a contracting path (encoder) that captures context and a symmetric expanding path (decoder) that enables precise localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ebf39b-ea57-4e4c-a32b-4e715ede5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bde56da-5e40-4dac-bece-50357e32dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73cb9c0b-cb02-40e7-a16c-fa9e75ae1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, pretrained=True, out_channels=12):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = vgg16_bn(pretrained=pretrained).features\n",
    "        self.block1 = nn.Sequential(*self.encoder[:6])\n",
    "        self.block2 = nn.Sequential(*self.encoder[6:13])\n",
    "        self.block3 = nn.Sequential(*self.encoder[13:20])\n",
    "        self.block4 = nn.Sequential(*self.encoder[20:27])\n",
    "        self.block5 = nn.Sequential(*self.encoder[27:34])\n",
    "\n",
    "        self.bottleneck = nn.Sequential(*self.encoder[34:])\n",
    "        self.conv_bottleneck = conv(512, 1024)\n",
    "\n",
    "        self.up_conv6 = up_conv(1024, 512)\n",
    "        self.conv6 = conv(512 + 512, 512)\n",
    "        self.up_conv7 = up_conv(512, 256)\n",
    "        self.conv7 = conv(256 + 512, 256)\n",
    "        self.up_conv8 = up_conv(256, 128)\n",
    "        self.conv8 = conv(128 + 256, 128)\n",
    "        self.up_conv9 = up_conv(128, 64)\n",
    "        self.conv9 = conv(64 + 128, 64)\n",
    "        self.up_conv10 = up_conv(64, 32)\n",
    "        self.conv10 = conv(32 + 64, 32)\n",
    "        self.conv11 = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contractive Path\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "\n",
    "        bottleneck = self.bottleneck(block5)\n",
    "        x = self.conv_bottleneck(bottleneck)\n",
    "        # Expansive Path\n",
    "        x = self.up_conv6(x)\n",
    "        x = torch.cat([x, block5], dim=1)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = self.up_conv7(x)\n",
    "        x = torch.cat([x, block4], dim=1)\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        x = self.up_conv8(x)\n",
    "        x = torch.cat([x, block3], dim=1)\n",
    "        x = self.conv8(x)\n",
    "\n",
    "        x = self.up_conv9(x)\n",
    "        x = torch.cat([x, block2], dim=1)\n",
    "        x = self.conv9(x)\n",
    "\n",
    "        x = self.up_conv10(x)\n",
    "        x = torch.cat([x, block1], dim=1)\n",
    "        x = self.conv10(x)\n",
    "\n",
    "        x = self.conv11(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd3854-084f-40ab-8bb2-94e5e4cdaf3a",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "### Initializes the U-Net model.\n",
    "   \n",
    "Takes two parameters:\n",
    "\n",
    "    - pretrained: A boolean indicating whether to use a pretrained VGG16 model.\n",
    "    - out_channels: The number of output channels for the final segmentation map (e.g., 12 classes for segmentation).\n",
    "    \n",
    "### Encoder (Contractive Path)\n",
    "\n",
    "    - The encoder part uses the features from a VGG16 model with batch normalization (vgg16_bn).\n",
    "    - The encoder is divided into five blocks, each consisting of several convolutional layers that progressively reduce the spatial dimensions while increasing the number of feature channels.\n",
    "    \n",
    "## Bottleneck\n",
    "\n",
    "    - The bottleneck section takes the deepest layers of the encoder.\n",
    "    - A convolution layer `conv_bottleneck` is applied to increase the number of feature channels from 512 to 1024, allowing the network to learn more complex features.\n",
    "    \n",
    "\n",
    "## Decoder (Expansive Path)\n",
    "\n",
    "The decoder consists of up-convolution (or transposed convolution) layers followed by concatenation with corresponding encoder features to retain spatial information:\n",
    "\n",
    "    - Each up_conv layer increases the spatial dimensions (upsampling).\n",
    "    - The output of each up-convolution is concatenated with the corresponding feature map from the encoder (skip connections).\n",
    "    - This helps the model learn both high-level features from deeper layers and low-level features from shallower layers.\n",
    "    - Finally, conv11 reduces the number of channels to out_channels (e.g., for multi-class segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c184e7-6c3b-4236-893e-3e134d8b0fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchvision.models.vgg.vgg16_bn(*, weights: Optional[torchvision.models.vgg.VGG16_BN_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fef1a14-c584-4f59-ae6a-0feb71c80b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ce = nn.CrossEntropyLoss()   # Applies softmax to output logits --> converts into class probabilities --> calculates neg. log likelihood loss between pred and true class label!!\n",
    "\n",
    "def UnetLoss(preds, targets):\n",
    "    ce_loss = ce(preds, targets)\n",
    "    acc = (torch.max(preds, 1)[1] == targets).float().mean()\n",
    "    #  (torch.max(preds, 1)[1] returns the indices of the maximum values along the class dimension (i.e., the predicted class for each pixel). \n",
    "    #  The 1 indicates that we're looking along the columns (the class dimension).\n",
    "    #  if preds class == targets return 1 --> change to float --> take mean to keep score between 0 and 1\n",
    "    return ce_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9aea8bb-8d15-4a8a-a3b7-f136bb65c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEngine():\n",
    "    def train_batch(model, data, optimizer, criterion):\n",
    "        model.train()\n",
    "\n",
    "        ims, ce_masks = data\n",
    "        _masks = model(ims)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, acc = criterion(_masks, ce_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item(), acc.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate_batch(model, data, criterion):\n",
    "        model.eval()\n",
    "\n",
    "        ims, masks = data\n",
    "        _masks = model(ims)\n",
    "\n",
    "        loss, acc = criterion(_masks, masks)\n",
    "\n",
    "        return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "845967f7-fa7f-461c-abed-1ed748f6c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def make_model():\n",
    "    model = UNet().to(args.cuda)\n",
    "    criterion = UnetLoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f025fdad-495f-4d49-942f-c434ae165bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /home/jupyter-dsai-st123439/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528M/528M [00:47<00:00, 11.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num. of parametes: 29311308\n",
      "Total num. of Trainable parametes: 29311308\n"
     ]
    }
   ],
   "source": [
    "model, criterion, optimizer = make_model()\n",
    "# Total num. of parametes\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# Total num. of \"trainable\" parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total num. of parametes: {num_params}')\n",
    "print(f'Total num. of Trainable parametes: {num_trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4372a-41e0-44f1-a516-f218766e47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    for epoch in range(args.epochs):\n",
    "        print(\"####################\")\n",
    "        print(f\"       Epoch: {epoch}   \")\n",
    "        print(\"####################\")\n",
    "\n",
    "        for batch_idx, data in tqdm(enumerate(trn_dl), total=len(trn_dl), leave=False):\n",
    "            train_loss, train_acc = TrainEngine.train_batch(model, data, optimizer, criterion)\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                # Print training information inline instead of calling a function\n",
    "                step = epoch * len(trn_dl) + batch_idx\n",
    "                print(f'Epoch [{epoch+1}/{args.epochs}], Step [{batch_idx}/{len(trn_dl)}], '\n",
    "                      f'Train Loss: {train_loss:.6f}, Accuracy: {train_acc:.6f}')\n",
    "\n",
    "        avg_val_acc = avg_val_loss = 0.0\n",
    "        for batch_idx, data in tqdm(enumerate(val_dl), total=len(val_dl)):\n",
    "            val_loss, val_acc = TrainEngine.validate_batch(model, data, criterion)\n",
    "\n",
    "            avg_val_loss += val_loss\n",
    "            avg_val_acc += val_acc\n",
    "\n",
    "        step = (epoch + 1) * len(trn_dl)\n",
    "        avg_val_loss /= len(val_dl)\n",
    "        avg_val_acc /= len(val_dl)\n",
    "        print(f'Val: Average loss: {avg_val_loss:.4f}, Accuracy: {avg_val_acc:.4f}')\n",
    "        print()\n",
    "\n",
    "    # Save the model and optimizer states after training is complete\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, 'unet.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af5080-3d59-496c-b4f7-d6a100c5f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22232f-9307-42dc-b5fe-7d4e3c2c61dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and save results\n",
    "for bx, data in tqdm(enumerate(val_dl), total=len(val_dl)):\n",
    "    im, mask = data\n",
    "    _mask = model(im)\n",
    "    _, _mask = torch.max(_mask, dim=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(im[0].permute(1, 2, 0).detach().cpu()[:, :, 0])\n",
    "    plt.savefig(f'original_image_{bx}.jpg')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(mask.permute(1, 2, 0).detach().cpu()[:, :, 0])\n",
    "    plt.savefig(f'original_mask_{bx}.jpg')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(_mask.permute(1, 2, 0).detach().cpu()[:, :, 0])\n",
    "    plt.savefig(f'predicted_mask_{bx}.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b5fde-e570-4a50-a71f-5f485f402b96",
   "metadata": {},
   "source": [
    "## IMPLEMENT SAM (SEGMENT ANYTHING MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36def362-6103-46bf-aeb7-4d4d2ac8221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Follow the blog post! - Choose a dataset to segment!\n",
    "\n",
    "## https://medium.com/@hasfatauil12/sam-segment-anything-model-d4f541165f6b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
